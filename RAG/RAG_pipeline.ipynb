{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Providing access to Google drive"
      ],
      "metadata": {
        "id": "B4mq54D5drVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ky6YKaYZ1nd",
        "outputId": "a354f8b1-8cea-4449-c46c-390937133067"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing dependencies"
      ],
      "metadata": {
        "id": "pTIfTe55eBzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_core langchain-groq langchain-community pypdf langchain-text-splitters langchain-chroma langchain-huggingface sentence-transformers langsmith streamlit"
      ],
      "metadata": {
        "id": "tA_mnBcDhnn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Ingestion"
      ],
      "metadata": {
        "id": "UzvQuy0atP5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load PDF content as Markdown"
      ],
      "metadata": {
        "id": "p74ocWbcF4I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader=PyPDFLoader(\"/content/drive/MyDrive/content/company_policy.pdf\")\n",
        "pages = loader.load()\n",
        "text_content = \"\\n\".join([page.page_content for page in pages])"
      ],
      "metadata": {
        "id": "Ky9e05TwFsN7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "218ce470-19f0-42e1-fbf7-bc14da793424"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Text Splitter"
      ],
      "metadata": {
        "id": "_v8oT--CGfKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1500,          # Max size of each chunk (in characters)\n",
        "    chunk_overlap=200,        # Number of characters to overlap between chunks\n",
        "    length_function=len,      # Use Python's len() to measure length\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "uUcNnJNxGmWu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding and Storing Chunks"
      ],
      "metadata": {
        "id": "zToFQ-LOIlwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "\n",
        "# Initialize the Embedding Model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Create the Vector Database\n",
        "vector_db = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"./architecture_db\"\n",
        ")\n",
        "\n",
        "print(\"Vector database created and saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXY8zgb6JP6O",
        "outputId": "8e25d131-720b-45ba-fa90-39adaa27193d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector database created and saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LCEL Retrieval Chain"
      ],
      "metadata": {
        "id": "YkO34pLBKFrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a prompt template"
      ],
      "metadata": {
        "id": "wcpP4L3fKzON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Create the Prompt Template\n",
        "template = \"\"\"\n",
        "You are a helpful HR assistant who knows all internal company policies.\n",
        "Refer to each section of the document to answer the user's question.\n",
        "\n",
        "If you don't know the answer based on the context, just say you don't know.\n",
        "Keep the answer concise and professional.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "hM22SuzsKNtu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a Retriever"
      ],
      "metadata": {
        "id": "z23kQC45LIDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "rBiivFZSLOP5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observability with LangSmith tool"
      ],
      "metadata": {
        "id": "x7dcmQ5zezEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"RAG_pipeline\"\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "x3KzgeeHe7Wu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langsmith import traceable\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0,\n",
        "    api_key=userdata.get(\"GROQ_API_KEY\"),\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "@traceable\n",
        "def format_prompt(subject):\n",
        "    \"\"\"Converts a raw subject into a structured list of messages.\"\"\"\n",
        "    system_msg = SystemMessage(content=\"You are a helpful HR assistant who knows all internal company policies. Provide concise answers.\")\n",
        "    human_msg = HumanMessage(content=f\"Extract the necessary information from a particular section: {subject}\")\n",
        "    return [system_msg, human_msg]\n",
        "\n",
        "@traceable(run_type=\"llm\")\n",
        "def invoke_llm(messages):\n",
        "    \"\"\"Sends messages to LLM and returns the full response object.\"\"\"\n",
        "    response = llm.invoke(messages)\n",
        "    return response\n",
        "\n",
        "@traceable\n",
        "def parse_output(response):\n",
        "    \"\"\"Extracts the text content from the LLM's complex response object.\"\"\"\n",
        "    return response.content.strip()\n",
        "\n",
        "@traceable\n",
        "def run_pipeline(subject):\n",
        "    \"\"\"Orchestrates the flow from input to final answer.\"\"\"\n",
        "    messages = format_prompt(subject)\n",
        "    response = invoke_llm(messages)\n",
        "    return parse_output(response)"
      ],
      "metadata": {
        "id": "7g6hCsYHgQT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recording response in the notebook"
      ],
      "metadata": {
        "id": "6oS_z05r5KNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result=run_pipeline(\"With reference to the Conditions of Employment, under what circumstances will the overtime of an employee be considered valid?\")\n",
        "print(f\"Answer: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGBqpa3Z5NK7",
        "outputId": "0acb2323-154c-4ab6-e1f4-b036e6a2c109"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: According to our company's Conditions of Employment (Section 5.3), overtime will be considered valid under the following circumstances:\n",
            "\n",
            "- The overtime is approved in advance by the employee's supervisor or manager.\n",
            "- The overtime is necessary to meet business needs or to complete a project deadline.\n",
            "- The employee has been given adequate notice of the overtime requirement.\n",
            "- The overtime does not exceed 10 hours per week, unless otherwise approved by HR.\n",
            "- The employee is paid at a rate of 1.5 times their regular hourly rate for all overtime worked.\n",
            "\n",
            "Please note that these conditions may be subject to change, and employees should refer to the most up-to-date version of the Conditions of Employment for the latest information.\n"
          ]
        }
      ]
    }
  ]
}